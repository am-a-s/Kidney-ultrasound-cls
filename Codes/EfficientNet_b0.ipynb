{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2LA9esaCcpf"
   },
   "source": [
    "### Import\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xbm8h2DbCbN0"
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from PIL import Image\n",
    "import re\n",
    "from torchvision import transforms, models\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2848,
     "status": "ok",
     "timestamp": 1727281254334,
     "user": {
      "displayName": "a se",
      "userId": "15717729663052586629"
     },
     "user_tz": -210
    },
    "id": "MSRvuUwFGfp5",
    "outputId": "a37950c7-ac51-43fa-feb0-95acc60cd610"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZPo3fD6GHZg"
   },
   "source": [
    "### Difine Seed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rWQtMbfoGJ6w"
   },
   "source": [
    "# Set a fixed seed value for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Set the seed for Python's built-in random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Ensure that the seed is set for CUDA as well if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "# Additional configuration for determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eazekJnCo9Q"
   },
   "source": [
    "### Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6uzDVZPgCsK-"
   },
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "\n",
    "        image = Image.open(file_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Function to extract patient ID from filenames (for k-fold cross validation)\n",
    "def extract_patient_id(filename):\n",
    "    match = re.match(r'([\\w\\s]+-\\d+of\\d+)', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
    "\n",
    "# Function to load data and split into file paths, labels, and patient IDs\n",
    "def load_data(root_dir, classes):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    patient_ids = []\n",
    "\n",
    "    for class_index, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            file_path = os.path.join(class_dir, filename)\n",
    "            file_paths.append(file_path)\n",
    "            labels.append(class_index)\n",
    "            patient_ids.append(extract_patient_id(filename))\n",
    "\n",
    "    return file_paths, labels, patient_ids\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t16MfAgkDibI"
   },
   "source": [
    "# Define root directory and classes\n",
    "root_dir = \"/content/drive/MyDrive/projects/Kidney/JPG_480/train\"\n",
    "test_root_dir = \"/content/drive/MyDrive/projects/Kidney/JPG_480/test\"\n",
    "test_root_dir_2 = \"/content/drive/MyDrive/projects/Kidney/JPG_480/val\"\n",
    "classes = ['Good', 'Bad', 'Null']\n",
    "\n",
    "# Load file paths, labels, and patient IDs\n",
    "file_paths, labels, patient_ids = load_data(root_dir, classes)\n",
    "test_file_paths, test_labels, test_patient_ids = load_data(test_root_dir, classes)\n",
    "test_file_paths_2, test_labels_2, test_patient_ids_2 = load_data(test_root_dir_2, classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1727281254334,
     "user": {
      "displayName": "a se",
      "userId": "15717729663052586629"
     },
     "user_tz": -210
    },
    "id": "iAPE-HOuMOTR",
    "outputId": "24fa6c9a-f556-457a-eb40-d130685b2eb2"
   },
   "source": [
    "# Create a test dataset\n",
    "test_dataset = CustomDataset(test_file_paths, test_labels, transform=transform)\n",
    "test_dataset_2 = CustomDataset(test_file_paths_2, test_labels_2, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader_2 = DataLoader(test_dataset_2, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4GYwrHACtAw"
   },
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_Zb-jYfCx4_"
   },
   "source": [
    "# Initialize GroupKFold\n",
    "num_folds = 5\n",
    "gkf = GroupKFold(n_splits=num_folds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1XFVq0WCzso"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K6UVuTMfC2J5"
   },
   "source": [
    "class ModifiedEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob=0.6):\n",
    "        super(ModifiedEfficientNet, self).__init__()\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)  # EfficientNet-b0 variant\n",
    "        num_features = self.efficientnet.classifier[1].in_features  # Get the number of input features\n",
    "        self.efficientnet.classifier = nn.Sequential(  # Replace classifier with custom layers\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mIkzpFzC3BP"
   },
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qsD7TPgFC7JD"
   },
   "source": [
    "fold_results = []  # To store the last validation accuracy of each fold\n",
    "best_model_per_fold = None  # To store the best model based on the last validation accuracy\n",
    "best_fold_acc = 0.0  # Track the highest validation accuracy among the last epochs of all folds\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "# Total number of unique patients across the dataset\n",
    "total_unique_patients = len(set(patient_ids))\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TRwkTrfIKSp"
   },
   "source": [
    "### Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6B3TDx39IAkp"
   },
   "source": [
    "# Function to train the model for a single epoch\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Function to validate the model\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lImurJMC8Fn"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(file_paths, labels, groups=patient_ids)):\n",
    "    print(f'Fold {fold+1}/{num_folds}')\n",
    "\n",
    "    # Get patient IDs for training and validation sets\n",
    "    train_patient_ids = [patient_ids[i] for i in train_idx]\n",
    "    val_patient_ids = [patient_ids[i] for i in val_idx]\n",
    "\n",
    "    # Check for overlap between training and validation patient IDs\n",
    "    overlap = set(train_patient_ids) & set(val_patient_ids)\n",
    "    if overlap:\n",
    "        print(f\"Overlap detected in fold {fold+1}! Overlapping patient IDs: {overlap}\")\n",
    "    else:\n",
    "        print(f\"No overlap between training and validation patient sets in fold {fold+1}.\")\n",
    "\n",
    "    # Calculate the percentage of unique patients in the validation set\n",
    "    unique_val_patients = len(set(val_patient_ids))\n",
    "    val_percentage = (unique_val_patients / total_unique_patients) * 100\n",
    "    print(f'Percentage of patients in validation set for fold {fold+1}: {val_percentage:.2f}%')\n",
    "\n",
    "    # Create training and validation datasets for this fold\n",
    "    train_dataset = CustomDataset([file_paths[i] for i in train_idx], [labels[i] for i in train_idx], transform=transform)\n",
    "    val_dataset = CustomDataset([file_paths[i] for i in val_idx], [labels[i] for i in val_idx], transform=transform)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Initialize the model for this fold (reinitialize weights)\n",
    "    model = ModifiedEfficientNet(num_classes=num_classes, dropout_prob=0.5)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.000714)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # Train and validate for one epoch\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Save the last validation accuracy of the current fold\n",
    "    fold_results.append(val_acc.item())\n",
    "\n",
    "    print(f'Last Validation Accuracy for Fold {fold+1}: {val_acc:.4f}')\n",
    "\n",
    "    # Track the best model and accuracy among the last validation accuracies\n",
    "    if val_acc > best_fold_acc:\n",
    "        best_fold_acc = val_acc\n",
    "        best_model_per_fold = copy.deepcopy(model.state_dict())  # Store the best model weights\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_acc = sum(fold_results) / len(fold_results)\n",
    "print(f'Average Accuracy across all folds: {average_acc:.4f}')\n",
    "\n",
    "# Print the best validation accuracy among the last validation accuracies\n",
    "print(f'Best Validation Accuracy among all folds: {best_fold_acc:.4f}')\n",
    "\n",
    "# Load the best model weights from the fold with the highest last validation accuracy\n",
    "model.load_state_dict(best_model_per_fold)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f574SULKvJaG",
    "executionInfo": {
     "status": "error",
     "timestamp": 1727281823119,
     "user_tz": -210,
     "elapsed": 568790,
     "user": {
      "displayName": "a se",
      "userId": "15717729663052586629"
     }
    },
    "outputId": "6b52b68b-6ce3-4a17-c426-5eed5ee14abb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NurIix1kC-qn"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SwOMuLgODALb"
   },
   "source": [
    "# Evaluate the model on the new test patient dataset\n",
    "model.eval()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_2:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "accuracy_merged = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "# Print accuracy for merged classes\n",
    "print(f\"Accuracy : {accuracy_merged:.4f}\")\n",
    "\n",
    "# Display the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Good', 'Bad', 'Null'],\n",
    "            yticklabels=['Good', 'Bad', 'Null'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hMKsCNyFWa-i"
   },
   "source": [
    "# Compute precision, recall, and F1-score\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "cls_report = classification_report(true_labels, predicted_labels, target_names=['Good', 'Bad', 'Null'])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(cls_report)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a2LA9esaCcpf",
    "lZPo3fD6GHZg",
    "_eazekJnCo9Q",
    "g4GYwrHACtAw",
    "4mIkzpFzC3BP",
    "2TRwkTrfIKSp",
    "NurIix1kC-qn"
   ],
   "gpuType": "T4",
   "provenance": [],
   "authorship_tag": "ABX9TyOp6NPQqH2l98yjQdi85iXA"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
